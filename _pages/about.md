---
permalink: /
title: ""
excerpt: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

{% if site.google_scholar_stats_use_cdn %}
{% assign gsDataBaseUrl = "https://cdn.jsdelivr.net/gh/" | append: site.repository | append: "@" %}
{% else %}
{% assign gsDataBaseUrl = "https://raw.githubusercontent.com/" | append: site.repository | append: "/" %}
{% endif %}
{% assign url = gsDataBaseUrl | append: "google-scholar-stats/gs_data_shieldsio.json" %}

<span class='anchor' id='about-me'></span>

Hi! Iâ€™m Ruoxuan Feng (å†¯è‹¥è½©, E-mail: fengruoxuan@ruc.edu.cn). I am a second year master student in <a href="https://gewu-lab.github.io/">GeWu-Lab</a> at <a href="http://ai.ruc.edu.cn/">Gaoling School of Artificial Intelligence, Renmin University of China</a>. I am advised by Prof. <a href="https://dtaoo.github.io/">Di Hu</a>. I received my bachelor's degree from <a href="https://csee.hnu.edu.cn/">College of Computer Science and Electronic Engineering, Hunan University</a> in 2023. Now my research interests focus on multi-modal emboided AI. I'm looking for a PhD position in 2026 Fall. If you are interested in my research, please email me at <a href="mailto:fengruoxuan@ruc.edu.cn">fengruoxuan@ruc.edu.cn</a>.


# ğŸ”¥ News
- *2025.04*: &nbsp;ğŸ‰ğŸ‰ One paper accepted by SIGIR, thanks to all co-authors!
- *2025.02*: &nbsp;ğŸ‰ğŸ‰ One paper accepted by CVPR, thanks to all co-authors!
- *2025.01*: &nbsp;ğŸ‰ğŸ‰ One paper accepted by ICLR, thanks to all co-authors!
- *2024.09*: &nbsp;ğŸ‰ğŸ‰ One paper accepted by CoRL, thanks to all co-authors! 
- *2024.07*: &nbsp;ğŸ‰ğŸ‰ One paper accepted by ECCV, thanks to all co-authors! 
- *2024.02*: &nbsp;ğŸ‰ğŸ‰ One paper accepted by CVPR, thanks to all co-authors!
- *2023.02*: &nbsp;ğŸ‰ğŸ‰ One paper accepted by ICASSP, thanks to all co-authors!


# ğŸ“ Publications 
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">CVPR 2025</div><img src='images/cvpr25.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

**<font size=4>Phoenix: A Motion-based Self-Reflection Framework for Fine-grained Robotic Action Correction</font>**

Wenke Xia, **Ruoxuan Feng**, Dong Wang, Di Hu

IEEE Conference on Computer Vision and Pattern Recognition (**CVPR**) 2025

[\[Paper\]](https://arxiv.org/abs/2504.14588) \| [\[Code\]](https://github.com/GeWu-Lab/Motion-based-Self-Reflection-Framework) \| [\[Project\]](https://xwinks.github.io/motion_instruction_for_correction/)
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICLR 2025</div><img src='images/iclr25.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

**<font size=4>AnyTouch: Learning Unified Static-Dynamic Representation across Multiple Visuo-tactile Sensors</font>**

**Ruoxuan Feng**, Jiangyu Hu, Wenke Xia, Tianci Gao, Ao Shen, Yuhao Sun, Bin Fang, Di Hu

International Conference on Learning Representations (**ICLR**) 2025

[\[Paper\]](https://openreview.net/pdf?id=XToAemis1h) \| [\[Code\]](https://github.com/GeWu-Lab/AnyTouch) \| [\[Project\]](https://gewu-lab.github.io/AnyTouch/)
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">CoRL 2024</div><img src='images/corl24.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

**<font size=4>Play to the Score: Stage-Guided Dynamic Multi-Sensory Fusion for Robotic Manipulation</font>**

**<font color=red>Oral Presentation</font>**

**Ruoxuan Feng**, Di Hu, Wenke Ma, Xuelong Li

Conference on Robot Learning (**CoRL**) 2024

[\[Paper\]](https://arxiv.org/abs/2408.01366) \| [\[Code\]](https://github.com/GeWu-Lab/MS-Bot) \| [\[Project\]](https://gewu-lab.github.io/MS-Bot/)
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">SIGIR 2025</div><img src='images/sigir25.jpg' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

**<font size=4>MGIPF: Multi-Granularity Interest Prediction Framework for Personalized Recommendation</font>**

**Ruoxuan Feng\***, Zhen Tian\*, Qiushi Peng, Jiaxin Mao, Xin Zhao, Di Hu, Changwang Zhang

International ACM SIGIR Conference on Research and Development in Information Retrieval (**SIGIR**) 2025

[\[Paper\]]() \| [\[Code\]](https://github.com/GeWu-Lab/MGIPF)
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ECCV 2024</div><img src='images/eccv24.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

**<font size=4>Diagnosing and Re-learning for Balanced Multimodal Learning</font>**

Yake Wei, Siwei Li, **Ruoxuan Feng**, Di Hu

European Conference on Computer Vision (**ECCV**) 2024

[\[Paper\]](https://arxiv.org/abs/2407.09705) \| [\[Code\]](https://github.com/GeWu-Lab/Diagnosing_Relearning_ECCV2024)
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">CVPR 2024</div><img src='images/cvpr24.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

**<font size=4>Enhancing Multimodal Cooperation via Sample-level Modality Valuation</font>**

Yake Wei, **Ruoxuan Feng**, Zihe Wang, Di Hu

IEEE Conference on Computer Vision and Pattern Recognition (**CVPR**) 2024

[\[Paper\]](https://arxiv.org/abs/2309.06255) \| [\[Code\]](https://github.com/GeWu-Lab/Valuate-and-Enhance-Multimodal-Cooperation)
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICASSP 2023</div><img src='images/icassp23.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

**<font size=4>MMCosine: Multi-Modal Cosine Loss Towards Balanced Audio-Visual Fine-Grained Learning</font>**

Ruize Xu, **Ruoxuan Feng**, Shi-xiong Zhang, Di Hu

IEEE International Conference on Acoustics, Speech and Signal Processing (**ICASSP**) 2023

[\[Paper\]](https://arxiv.org/abs/2303.05338) \| [\[Code\]](https://github.com/GeWu-Lab/MMCosine_ICASSP23) \| [\[Project\]](https://gewu-lab.github.io/MMCosine/)
</div>
</div>

# ğŸ“ Preprint
<div class='paper-box'><div class='paper-box-image'><div><img src='images/arxiv23.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

**<font size=4>Revisiting Pre-training in Audio-Visual Learning</font>**

**Ruoxuan Feng**, Wenke Xia, Di Hu

arXiv 2302.03533

[\[Paper\]](https://arxiv.org/abs/2302.03533) \| [\[Code\]](https://github.com/GeWu-Lab/Revisiting-Pre-training-in-Audio-Visual-Learning) 
</div>
</div>

# ğŸ’» Internships
- *2025.05 - now*, Beijing Academy of Artificial Intelligence (BAAI), Beijing.

# ğŸ“– Educations
- *2023.09 - now*, Master, Gaoling School of Artificial Intelligence, Renmin University of China.
- *2019.09 - 2023.06*, Undergraduate, College of Computer Science and Electronic Engineering, Hunan University.

# ğŸ– Honors and Awards
- *2023.06* Outstanding graduate of Hunan University. 
- *2020.12* National Scholarship. 
